{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e70690",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/leolorenzoii/ml2_interpretability/blob/main/notebooks/01_Model_Interpretability_and_Shapley_Values.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bc83a",
   "metadata": {},
   "source": [
    "# Introduction to Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44863d11",
   "metadata": {},
   "source": [
    "Throughout the **Machine Learning 1** course, we learned all about different machine learning algorithms- how they work, how we can hypertune their hyperparameters, and how we can select the optimal model through a cross validation strategy. In the end, we can now train a machine learning model and use it to make predictions on unseen data (see Figure <a href='#fig:ml-nutshell'>1</a>).\n",
    "\n",
    "<a name='fig:ml-nutshell'></a>\n",
    "<div>\n",
    "<img src=\"images/ml-nutshell.png\" align=\"left\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 1. Training and testing a machine learning model in a nutshell.</b><br>\n",
    "        We learned how to train a machine learning model and use it to predict unseen data.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "However, our methodology is currently geared more towards only optimizing the predictive power of our models and less on how we can use our models for inference. In times where stakeholders require us to explain the predictions of our machine learning model, our current methodology will be insufficient. In particular, we lack the capacity to answer questions such as:\n",
    "\n",
    "- How does one (or more) feature impact the predictions of the model?\n",
    "- What is the role that each feature value play in each individual predictions?\n",
    "- How can we explain the model's predictions in a more useful manner for our stakeholders?\n",
    "\n",
    "We will deal with methods that improves the explainability of our models in the next series of notebooks. For our first notebook on interpretability, a brief introduction on model interpretability and its importance will be emphasized. We also show in the concluding section of this notebook how we can incorporate these explainability methods on our machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff86f",
   "metadata": {},
   "source": [
    "## What is model interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad8798",
   "metadata": {},
   "source": [
    "In Christoph Molnar's [Interpretable Machine Learning](#ref:molnar) book [[1]](#ref:molnar), he collated two definitions of **interpretability**. A non-mathematical one:\n",
    "\n",
    "> *Interpretability is the degree to which a* ***human*** *can* ***understand*** *the cause of a* ***decision***. [[2]](#ref:miller)\n",
    "\n",
    "And a mathematical one:\n",
    "\n",
    "> *Interpretability is the degree to which a* ***human*** *can consistently* ***predict*** *the* ***model‚Äôs result***. [[3]](#ref:kim)\n",
    "\n",
    "In both definitions we see three important elements: the **human**, the **understanding**, and the **decision**. Thus, in the same way, when we construct our definition for *interpretability* in the context of machine learning:\n",
    "\n",
    "> ***Model interpretability*** *refers to the degree in which the behaviors and tendencies of statistical and machine learning models are understandable to humans.*\n",
    "\n",
    "Notice here that there is a *flexibility* in the definition of model interpretability. Indeed, defining how explicable a machine learning model is depends on the needs and requirements of a project and the different stakeholders.\n",
    "\n",
    "Nevertheless, as ethical machine learning practitioners and data science leaders, model interpretability MUST be integrated as early as possible in the development process and should NOT just be taken as an afterthought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275578aa",
   "metadata": {},
   "source": [
    "## Why interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0b6d2",
   "metadata": {},
   "source": [
    "Now, one might ask, why bother with interpretability? Wouldn't a high model performance would ultimately yield to higher business value? This is quite a arguable topic! In fact, in 2017, the [Neural Information Processing Systems](https://nips.cc/) conference in 2017 had its first ever ***The Great AI Debate*** with the topic ***Is interpretability necessary for machine learning?*** [[4]](#ref:great-ai-debate) *(you are highly encourage to watch this engaging and insightful discussion* üôÇ).\n",
    "\n",
    "In the video, it was shown that model interpretability is crucial especially for some applications where quirky patterns from the data may be learned by the model. Indeed, having an interpretability pipeline in your project gives you, the Data Scientist, the ability to debug your model and identify issues early on. Thus, giving you a chance to improve your model. Furthermore, it has added benefits for other stakeholders that is interested and affected by your machine learning model (see Figure [2](#fig:stakeholders)).\n",
    "\n",
    "<a name='fig:stakeholders'></a>\n",
    "<div>\n",
    "<img src=\"images/interpretability-stakeholders.png\" align=\"left\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 2. Benefits of model interpretability to various stakeholders of a machine learning project.</b><br>\n",
    "           Model interpretability benefits the data scientist, business decision makers, approving authorities, and business customers.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "For **data scientists**, being able to explain the model to other stakeholders is also one benefit of having model explicability. The better you can explain the model to other people in the business, the greater its chance of being adopted and the trust given to the model by other stakeholders. With model interpretability, **business executives** now has the option to provide transparency to its end-users. Furthermore, it helps them justify the business case for the investment and identify other potential extensions and business use-case for the project. **Approving authorities** also benefit from model interpretability, by having a clear understanding of the risk the business is going to take in adopting the model, understanding the impact of the model decisions to humans, and anticipate any legal or regulatory issues that the model may face. Finally, **customer** experience and decision making can also be improved if they understand why a model gives a certain prediction.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "\n",
    "Here, we outlined the benefits of having an interpretability pipeline in our machine learning projects. Can you think of cases where having model interpretability is NOT preferred? Give a particular instance where having model interpretability can do more harm than good.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33556941",
   "metadata": {},
   "source": [
    "## What makes a good explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dfe86",
   "metadata": {},
   "source": [
    "Before we begin explaining predictions of machine learning models, it helps to understand what makes an explanation good an acceptable for humans. We are making explanations, after all, for a humans to be digested. This help us better frame the model explanations we get from model interpretability methods.\n",
    "\n",
    "Here are some of the few important characteristics of a good explanation *(see Chapter 3.6 of Molnar's Interpretable Machine Learning for a complete list [[1]](#ref:molnar))*:\n",
    "\n",
    "1. **Contrastive** - Model explanations must be able to answer *why a given prediction has been made in place of another prediction*. For example, in a model that recommends whether an individual be given a loan or not, a good explanation must be able to tell *what factors should/could I change to alter the model prediction*.\n",
    "2. **Selective** - We *humans are only capable to comprehend 2 to 3 variables at a time*. Thus, a good model explanation must be able to *list the important drivers to explain an outcome*. Imagine having an interpretable model such as linear regression or decision trees, but an explanation that looks at hundreds or thousands of variables, it would be really hard for any human to digest that explanation!\n",
    "3. **Consistent with prior beliefs** - Model explanations are greatly affected by how people perceived them. As such, when *an explanation is consistent with the prior beliefs of an individual*, they tend to favor such explanation ( also known as **confirmation bias**). This is not to discredit any novel or serendipitous discoveries of the model explanation. However, having an explanation that is in line with a domain expert, helps in the model's adoption. Furthermore, if a model is found to exhibit behavior inconsistent with the domain expert's belief, we can enforce constraints on the model or use a linear model that has the required property.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "\n",
    "Among the interpretability methods that you currently know, can you create model explanations that satisfies all of the three characteristics we discussed above?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1295d",
   "metadata": {},
   "source": [
    "## Types of Explainability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8a02b",
   "metadata": {},
   "source": [
    "Model interpretability methods can be classified according to three different criteria (see Figure 5) [[5]](#ref:bbox-peek):\n",
    "\n",
    "<a name='fig:taxonomy'></a>\n",
    "<div>\n",
    "<img src=\"images/taxonomy.png\" align=\"left\" width=\"550\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 3. Taxonomy of different explainability methods.</b><br>\n",
    "           Model interpretability or explainability methods can be intrinsic or post-hoc, model-specific or model-agnostic, local or global. Model-agnostic methods can be further classified whether they use surrogate models or are just visualizations of the behavior of the black-box model.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "First, we can classify whether the method stems from the model being **intrinsically** interpretable. If the model is not intrinsically interpretable, then the explanation method can be applied **post-hoc** or post-model training. Examples of intrinsically interpretable models include: Linear models, Decision Tree, and Rule-based models ([RuleFit](https://github.com/rohan-gt/rulefit) [[6]](#ref:rulefit)).\n",
    "\n",
    "We can further classify the method whether it is **model-specific** or **model-agnostic** (intrinsic explainability methods are model-specific by definition). Model agnostic means that the explainability method can be applied to any black box models.\n",
    "\n",
    "Finally, we can classify an explainability method whether it is a **global** explanation - i.e., it explains the whole model behavior (ex. feature importance and summary visualizations), or whether it is a **local** explanation, i.e., it explains a particular instance in the test or train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7283f",
   "metadata": {},
   "source": [
    "## Predictive vs Explanatory Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae2ca7",
   "metadata": {},
   "source": [
    "Two of the models we introduced in machine learning 1 can actually be considered as an intrinsically explainable method, i.e., Decision Trees and Linear Regression models. However, we've only presented them in the context of maximizing the prediction accuracy that we get from these models. For us to reframe their usage for explainability, we need to first differentiate what we mean by using models for prediction versus explanation [[7]](#ref:predict-explain).\n",
    "\n",
    "- **Predictive Modeling** - *process of applying a statistical model or machine learning model to data for the purpose of predicting new or future observations.* As such, the goal for predictive modeling is to minimize the combination of bias and estimation variance to obtain the required empirical precision.\n",
    "\n",
    "- **Explanatory Modeling** - *process of applying (usually) statistical models to data to validate hypothesis about the theoretical constructs of the data generation process.*. The focus for explanatory modeling is to minimize bias to obtain the most accurate representation of the underlying theory.\n",
    "\n",
    "To see how these differ, let's look at two different pipelines in the succeeding subsections - one will focus on predictive modeling (as usual), while the other would heavily focus on explanatory modeling.\n",
    "\n",
    "We'll set our data generation process to be linear for our analysis to emphasize the difference between the two pipeline (see Equation \\ref{eq:data-gen}).\n",
    "\n",
    "\\begin{equation}\n",
    "y =  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2^3 + \\epsilon \\tag{1} \\label{eq:data-gen}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7273df73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:06:19.759544Z",
     "start_time": "2023-01-25T17:06:19.268308Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sklearn make regressions allows us to generate a random regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate 2000 samples using the model:\n",
    "# y = b_2 X_2^3 + b_1 X_1 + 0.77\n",
    "# while adding 4.50 noise to the target.\n",
    "#\n",
    "# Notice we name the output data matrix as X_processed, this is because\n",
    "# we will take the cube root of second feature to simulate a linear\n",
    "# dependence on X_2^3.\n",
    "X_processed, y, coef = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    bias=0.77,\n",
    "    coef=True,\n",
    "    noise=4.50,\n",
    "    random_state=1337\n",
    ")\n",
    "X = np.array([X_processed[:, 0], np.cbrt(X_processed[:, 1])]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dd412",
   "metadata": {},
   "source": [
    "### Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6273d4",
   "metadata": {},
   "source": [
    "A predictive modeling pipeline would proceed as follows (for the model experimentation and evaluation step):\n",
    "\n",
    "1. Hold-out test set segregation - setting out a hold-out test set for evaluation\n",
    "2. Model selection - shortlisting which models to use\n",
    "3. Cross validation design - how to perform cross validation\n",
    "4. Hyperparameter tuning - deciding which optimal model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f25c7",
   "metadata": {},
   "source": [
    "Let's use 20% of the datapoints as our hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a13c800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:06:19.778292Z",
     "start_time": "2023-01-25T17:06:19.761883Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_holdout, y_trainval, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d7c03",
   "metadata": {},
   "source": [
    "Suppose we decided to use the following models:\n",
    "\n",
    "1. Ridge Regression\n",
    "2. Linear SVM\n",
    "3. Nonlinear SVM\n",
    "\n",
    "Then hypertune the `C` or `alpha` of the model along the range of `[1.e-05, 1.e-03, 1.e-01, 1.e+01, 1.e+03, 1.e+05]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61066228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:09:06.744043Z",
     "start_time": "2023-01-25T17:09:06.726784Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define C and alpha hyperparameter range\n",
    "C_range = np.logspace(-5, 5, 6)\n",
    "\n",
    "# Prepare the pipeline and parameter grid\n",
    "pipe = Pipeline([('clf', Ridge())])\n",
    "param_grid = [\n",
    "    {'clf': [Ridge(random_state=1337)], 'clf__alpha': C_range},\n",
    "    {'clf': [SVR()], 'clf__kernel': ['linear', 'rbf'], 'clf__C': C_range}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abe101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T16:37:25.188088Z",
     "start_time": "2023-01-25T16:37:25.180102Z"
    }
   },
   "source": [
    "Next, let's find the optimal model for this case using a 5-fold cross validation strategy using `r2` as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6712c271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:09:09.726861Z",
     "start_time": "2023-01-25T17:09:07.407095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is: {'clf': SVR(C=1000.0), 'clf__C': 1000.0, 'clf__kernel': 'rbf'}\n",
      "with an R^2 score of: 0.9789207127962802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid search using 5-fold CV with r2 scoring metric\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "grid_search.fit(X_trainval, y_trainval)\n",
    "print(f\"The best model is: {grid_search.best_params_}\")\n",
    "print(f\"with an R^2 score of: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862bd23",
   "metadata": {},
   "source": [
    "Using this model to evaluate the performance of our model to the test set, we get,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8774f726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:21:20.236929Z",
     "start_time": "2023-01-25T17:21:20.219419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914165852086331"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.score(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641c13c",
   "metadata": {},
   "source": [
    "At this point, we've attained the objective of our predictive modeling exercise. We've optimized towards empirical precision and found that our model's performance during cross validation is comparable with its performance on the hold-out test set. However, notice that our optimal model here is a non-linear SVR with an RBF kernel. In terms of predictive performance, this is the optimal model. However, for explicability, this model might not be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9fd483",
   "metadata": {},
   "source": [
    "### Explanatory Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575579ca",
   "metadata": {},
   "source": [
    "A crucial component of explanatory modeling is the statistical model that tries to recreate the underlying data generating process of the system of interest. This model not only has to be interpretable but the relationship between the target and predictive features should be clearly specified. As such, one of the common explanatory model to use are linear regression models. Simply put, a linear regression model predicts the target as a weighted sum of feature inputs.\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p + \\epsilon \\tag{2} \\label{eq:lin-reg}\n",
    "$$\n",
    "\n",
    "This feature of linear regression is both its strength and weakness. The simplicity of the linear regression model comes with several assumptions. Recall that these assumptions are [[8]](#ref:degroot):\n",
    "\n",
    "1. **Normality** - The conditional distribution of $y$ given predictors $x_i$ is a normal distribution.\n",
    "2. **Linearity** - The relationship between the outcome $y$ and features $x_i$ is linear.\n",
    "3. **Common Variance (Homoscedasticity)** - There is a parameter $\\sigma^2$ such that the conditional variance of $y$ given any $x_i$ is $\\sigma^2$.\n",
    "4. **Independence** - Observations are independent of each other.\n",
    "\n",
    "As such, the pipeline of explanatory modeling involves making sure that these assumptions should hold given that we are trying to use the linear regression model to explain how we've obtained our current observations. This can be summarized as follows:\n",
    "\n",
    "1. Ensuring gaussian outcomes - for non-Gaussian outcomes, a transformation is done on the outcome variable but the weighted sum of features is kept intact.\n",
    "2. Accounting for interactions - features may not be totally independent from one another. As such, we can perform feature-crossing to describe these interactions.\n",
    "3. Modeling nonlinear features - techniques such as feature interaction, feature bucketization, and GAMs (Generalized Additive Models) may be used to account for nonlinear features.\n",
    "\n",
    "**IMPORTANT!!** - One major difference between explanatory modeling and predictive modeling is that for explanatory model, we are only concerned with the full dataset. No hold-out test set nor cross validation evaluation strategy will be needed since we only focus on recreating the data generation process using our chosen statistical model.\n",
    "\n",
    "Let's take the case that we perform feature transformation to satisfy the assumption of the linear model when we describe our observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24002e28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T19:23:01.707690Z",
     "start_time": "2023-01-25T19:23:01.700404Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trans = np.array([X[:, 0], X[:, 1]**3]).T # this is magic! There is an \n",
    "                                            # arduous iterative process to get\n",
    "                                            # at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3058c",
   "metadata": {},
   "source": [
    "We then fit a linear regression on this transformed features to get our explanatory model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ced5f1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T19:51:36.552047Z",
     "start_time": "2023-01-25T19:51:36.541374Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_trans, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdd7c1",
   "metadata": {},
   "source": [
    "Using this linear model, we can then use this as basis for recreating how each feature contributes toward each corresponding outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7cc7eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T19:49:57.047472Z",
     "start_time": "2023-01-25T19:49:57.038175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.01817226, 87.00025353])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ea92b",
   "metadata": {},
   "source": [
    "Comparing this with the actual data generating process, we are not too very far off! Which means we've attained the objective of explanatory modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a9016d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T19:50:02.270219Z",
     "start_time": "2023-01-25T19:50:02.260905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.63936755, 87.21220272])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd7568",
   "metadata": {},
   "source": [
    "### But wait, there's more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50223c5d",
   "metadata": {},
   "source": [
    "We've established from the previous section the difference of the focus and goal of predictive versus explainatory modeling. Notice that more often than not, when we focus on predictive accuracy, we sacrifice theoretical accuracy for improved empirical precision. Indeed, when it comes to choosing the model with the highest predictive accuracy, the optimal model are the least interpretable. (see Figure <a href='#fig:accuracy-interpretability'>2</a>) [[9]](#ref:interpret-ml).\n",
    "\n",
    "<a name='fig:accuracy-interpretability'></a>\n",
    "<div>\n",
    "<img src=\"images/accuracy-interpretability-trade-off.PNG\" align=\"left\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 2. Machine learning model accuracy and interpretability tradeoff.</b><br>\n",
    "           Models that are highly accurate are the least interpretable, while models that are highly interpretable have a sub-par accuracy [<a href='#ref:interpret-ml'>9</a>].\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "However, what if our the business requirement insist that our models be strong in both predictive and explainative capability? Are there ways in which we can use the optimal model found during our predictive modeling pipeline but still be able to explain the predictions of our model?\n",
    "\n",
    "Yes, we can! By using model-agnostic post-hoc models! We demonstrate in the next few cells how we can accomplish this using SHAP (SHapley Additive exPlanations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79461ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:21:25.059444Z",
     "start_time": "2023-01-25T17:21:25.053091Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.KernelExplainer(best_model.predict, X)\n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa3f1fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:27:17.659200Z",
     "start_time": "2023-01-25T17:27:17.548392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x165.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values=shap_values, features=X, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339efb5b",
   "metadata": {},
   "source": [
    "Printing and comparing the actual mean absolute SHAP with the coefficient of the data generating process, we get,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2d3f92f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T17:45:17.579391Z",
     "start_time": "2023-01-25T17:45:17.570570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(|SHAP value|) [27.61341006 67.41490067]\n",
      "actual coef: [32.63936755 87.21220272]\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean(|SHAP value|) {abs(shap_values).mean(axis=0)}\")\n",
    "print(f\"actual coef: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9c6f2",
   "metadata": {},
   "source": [
    "Magnitude-wise, they may differ to some degree, but the general behavior wherein $X_2$ has a greater effect than $X_1$ was captured by our explainability method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ad57d",
   "metadata": {},
   "source": [
    "This is just one way we can use SHAP to explain any black box model we have trained during our model experimentation. We will discuss how SHAP works as well as explore the other explainatory functionalities of it during the post-hoc explanation methods session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088ddb7",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccff2fa",
   "metadata": {},
   "source": [
    "<a name='ref:molnar'></a> [1] Molnar, Christoph. ‚ÄúInterpretable machine learning. A Guide for Making Black Box Models Explainable‚Äù, 2019. https://christophm.github.io/interpretable-ml-book/.\n",
    "\n",
    "<a name='ref:miller'></a> [2] Miller, Tim. ‚ÄúExplanation in artificial intelligence: Insights from the social sciences.‚Äù *arXiv Preprint arXiv:1706.07269.* (2017)\n",
    "\n",
    "<a name='ref:kim'></a> [3] Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. ‚ÄúExamples are not enough, learn to criticize! Criticism for interpretability.‚Äù *Advances in Neural Information Processing Systems* (2016).\n",
    "\n",
    "<a name='ref:great-ai-debate'></a> [4] NeurIPS 2017. ‚ÄúThe Great AI Debate - NIPS2017 - Yann LeCun.‚Äù *YouTube*, uploaded by The Artificial Intelligence Channel, 1 February 2018, https://youtu.be/93Xv8vJ2acI.\n",
    "\n",
    "<a name='ref:bbox-peek'></a> [5] Adadi, Amina, and Mohammed Berrada. \"Peeking inside the black-box: a survey on explainable artificial intelligence (XAI).\" *IEEE access 6* (2018): 52138-52160.\n",
    "\n",
    "<a name='ref:rulefit'></a> [6] Friedman, Jerome H., and Bogdan E. Popescu. \"Predictive learning via rule ensembles.\" The Annals of Applied Statistics 2.3 (2008): 916-954.\n",
    "\n",
    "<a name='ref:predict-explain'></a> [7] Shmueli G. *To explain or to predict?*. Statistical science. 2010 Aug;25(3):289-310.\n",
    "\n",
    "<a name='ref:degroot'></a> [8] DeGroot MH, Schervish MJ. Probability and statistics. Pearson Education; 2012.\n",
    "\n",
    "<a name='ref:interpret-ml'></a> [9] Guo, Mengzhuo, et al. \"An interpretable machine learning framework for modelling human decision behavior.\" *arXiv preprint arXiv:1906.01233* (2019)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
